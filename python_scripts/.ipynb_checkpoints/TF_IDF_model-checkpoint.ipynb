{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from nltk import tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9533fb74b295>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'punkt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>useful</th>\n",
       "      <th>business_id</th>\n",
       "      <th>text</th>\n",
       "      <th>funny</th>\n",
       "      <th>user_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>cool</th>\n",
       "      <th>review_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'2013-05-07 04:34:36'</td>\n",
       "      <td>6.0</td>\n",
       "      <td>b'ujmEBvifdJM6h6RLv4wQIg'</td>\n",
       "      <td>total bill for this horrible service over $8gs...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>b'hG7b0MtEbXx5QzbzE6C_VA'</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'Q1sbwvVQXV2734tPgoKj4Q'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'2017-01-14 21:30:33'</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'NZnhc2sEQy3RmzKTZnqtwQ'</td>\n",
       "      <td>i adore travis at the hard rocks new kelly car...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'yXQM5uF2jS6es16SJzNHfg'</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'GJXCdrto3ASJOqKeVWPi6Q'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'2016-11-09 20:09:03'</td>\n",
       "      <td>3.0</td>\n",
       "      <td>b'WTqjgwHlXbSFevF32_DJVw'</td>\n",
       "      <td>i have to say that this office really has it t...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'n6-Gk65cPZL6Uz8qRm3NYw'</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'2TzJjDVDEuAW6MR5Vuc1ug'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'2018-01-09 20:56:38'</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'ikCg8xy5JIg_NGPx-MSIDA'</td>\n",
       "      <td>went in for a lunch steak sandwich was delicio...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'dacAIZ6fTM6mqwW5uxkskg'</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'yi0R0Ugj_xUx_Nek0-_Qig'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'2018-01-30 23:07:38'</td>\n",
       "      <td>7.0</td>\n",
       "      <td>b'b1b1eb3uo-w561D0ZfCEiQ'</td>\n",
       "      <td>today was my second out of three sessions i ha...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'ssoyf2_x0EQMed6fgHeMyQ'</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'11a8sVPMUFtaC7_ABRkmtw'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     date  useful                business_id  \\\n",
       "0  b'2013-05-07 04:34:36'     6.0  b'ujmEBvifdJM6h6RLv4wQIg'   \n",
       "1  b'2017-01-14 21:30:33'     0.0  b'NZnhc2sEQy3RmzKTZnqtwQ'   \n",
       "2  b'2016-11-09 20:09:03'     3.0  b'WTqjgwHlXbSFevF32_DJVw'   \n",
       "3  b'2018-01-09 20:56:38'     0.0  b'ikCg8xy5JIg_NGPx-MSIDA'   \n",
       "4  b'2018-01-30 23:07:38'     7.0  b'b1b1eb3uo-w561D0ZfCEiQ'   \n",
       "\n",
       "                                                text  funny  \\\n",
       "0  total bill for this horrible service over $8gs...    1.0   \n",
       "1  i adore travis at the hard rocks new kelly car...    0.0   \n",
       "2  i have to say that this office really has it t...    0.0   \n",
       "3  went in for a lunch steak sandwich was delicio...    0.0   \n",
       "4  today was my second out of three sessions i ha...    0.0   \n",
       "\n",
       "                     user_id  stars  cool                  review_id  \n",
       "0  b'hG7b0MtEbXx5QzbzE6C_VA'    1.0   0.0  b'Q1sbwvVQXV2734tPgoKj4Q'  \n",
       "1  b'yXQM5uF2jS6es16SJzNHfg'    5.0   0.0  b'GJXCdrto3ASJOqKeVWPi6Q'  \n",
       "2  b'n6-Gk65cPZL6Uz8qRm3NYw'    5.0   0.0  b'2TzJjDVDEuAW6MR5Vuc1ug'  \n",
       "3  b'dacAIZ6fTM6mqwW5uxkskg'    5.0   0.0  b'yi0R0Ugj_xUx_Nek0-_Qig'  \n",
       "4  b'ssoyf2_x0EQMed6fgHeMyQ'    1.0   0.0  b'11a8sVPMUFtaC7_ABRkmtw'  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = pd.read_csv('review_test_clean.csv')\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['text'] = reviews['text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['tokenized_reviews'] = reviews[\"text\"].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>useful</th>\n",
       "      <th>business_id</th>\n",
       "      <th>text</th>\n",
       "      <th>funny</th>\n",
       "      <th>user_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>cool</th>\n",
       "      <th>review_id</th>\n",
       "      <th>tokenized_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'2013-05-07 04:34:36'</td>\n",
       "      <td>6.0</td>\n",
       "      <td>b'ujmEBvifdJM6h6RLv4wQIg'</td>\n",
       "      <td>total bill for this horrible service over $8gs...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>b'hG7b0MtEbXx5QzbzE6C_VA'</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'Q1sbwvVQXV2734tPgoKj4Q'</td>\n",
       "      <td>[total, bill, for, this, horrible, service, ov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'2017-01-14 21:30:33'</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'NZnhc2sEQy3RmzKTZnqtwQ'</td>\n",
       "      <td>i adore travis at the hard rocks new kelly car...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'yXQM5uF2jS6es16SJzNHfg'</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'GJXCdrto3ASJOqKeVWPi6Q'</td>\n",
       "      <td>[i, adore, travis, at, the, hard, rocks, new, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'2016-11-09 20:09:03'</td>\n",
       "      <td>3.0</td>\n",
       "      <td>b'WTqjgwHlXbSFevF32_DJVw'</td>\n",
       "      <td>i have to say that this office really has it t...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'n6-Gk65cPZL6Uz8qRm3NYw'</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'2TzJjDVDEuAW6MR5Vuc1ug'</td>\n",
       "      <td>[i, have, to, say, that, this, office, really,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'2018-01-09 20:56:38'</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'ikCg8xy5JIg_NGPx-MSIDA'</td>\n",
       "      <td>went in for a lunch steak sandwich was delicio...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'dacAIZ6fTM6mqwW5uxkskg'</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'yi0R0Ugj_xUx_Nek0-_Qig'</td>\n",
       "      <td>[went, in, for, a, lunch, steak, sandwich, was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'2018-01-30 23:07:38'</td>\n",
       "      <td>7.0</td>\n",
       "      <td>b'b1b1eb3uo-w561D0ZfCEiQ'</td>\n",
       "      <td>today was my second out of three sessions i ha...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'ssoyf2_x0EQMed6fgHeMyQ'</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'11a8sVPMUFtaC7_ABRkmtw'</td>\n",
       "      <td>[today, was, my, second, out, of, three, sessi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     date  useful                business_id  \\\n",
       "0  b'2013-05-07 04:34:36'     6.0  b'ujmEBvifdJM6h6RLv4wQIg'   \n",
       "1  b'2017-01-14 21:30:33'     0.0  b'NZnhc2sEQy3RmzKTZnqtwQ'   \n",
       "2  b'2016-11-09 20:09:03'     3.0  b'WTqjgwHlXbSFevF32_DJVw'   \n",
       "3  b'2018-01-09 20:56:38'     0.0  b'ikCg8xy5JIg_NGPx-MSIDA'   \n",
       "4  b'2018-01-30 23:07:38'     7.0  b'b1b1eb3uo-w561D0ZfCEiQ'   \n",
       "\n",
       "                                                text  funny  \\\n",
       "0  total bill for this horrible service over $8gs...    1.0   \n",
       "1  i adore travis at the hard rocks new kelly car...    0.0   \n",
       "2  i have to say that this office really has it t...    0.0   \n",
       "3  went in for a lunch steak sandwich was delicio...    0.0   \n",
       "4  today was my second out of three sessions i ha...    0.0   \n",
       "\n",
       "                     user_id  stars  cool                  review_id  \\\n",
       "0  b'hG7b0MtEbXx5QzbzE6C_VA'    1.0   0.0  b'Q1sbwvVQXV2734tPgoKj4Q'   \n",
       "1  b'yXQM5uF2jS6es16SJzNHfg'    5.0   0.0  b'GJXCdrto3ASJOqKeVWPi6Q'   \n",
       "2  b'n6-Gk65cPZL6Uz8qRm3NYw'    5.0   0.0  b'2TzJjDVDEuAW6MR5Vuc1ug'   \n",
       "3  b'dacAIZ6fTM6mqwW5uxkskg'    5.0   0.0  b'yi0R0Ugj_xUx_Nek0-_Qig'   \n",
       "4  b'ssoyf2_x0EQMed6fgHeMyQ'    1.0   0.0  b'11a8sVPMUFtaC7_ABRkmtw'   \n",
       "\n",
       "                                   tokenized_reviews  \n",
       "0  [total, bill, for, this, horrible, service, ov...  \n",
       "1  [i, adore, travis, at, the, hard, rocks, new, ...  \n",
       "2  [i, have, to, say, that, this, office, really,...  \n",
       "3  [went, in, for, a, lunch, steak, sandwich, was...  \n",
       "4  [today, was, my, second, out, of, three, sessi...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "english_stop_words = stopwords.words('english')\n",
    "def remove_stop_words(corpus):\n",
    "    removed_stop_words = []\n",
    "    for review in corpus:\n",
    "        removed_stop_words.append(\n",
    "            ' '.join([word for word in review.split() \n",
    "                      if word not in english_stop_words])\n",
    "        )\n",
    "    return removed_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>useful</th>\n",
       "      <th>business_id</th>\n",
       "      <th>text</th>\n",
       "      <th>funny</th>\n",
       "      <th>user_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>cool</th>\n",
       "      <th>review_id</th>\n",
       "      <th>tokenized_reviews</th>\n",
       "      <th>no_stop_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'2013-05-07 04:34:36'</td>\n",
       "      <td>6.0</td>\n",
       "      <td>b'ujmEBvifdJM6h6RLv4wQIg'</td>\n",
       "      <td>total bill for this horrible service over $8gs...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>b'hG7b0MtEbXx5QzbzE6C_VA'</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'Q1sbwvVQXV2734tPgoKj4Q'</td>\n",
       "      <td>[total, bill, for, this, horrible, service, ov...</td>\n",
       "      <td>[total, bill, , , horrible, service, , $, 8gs,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'2017-01-14 21:30:33'</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'NZnhc2sEQy3RmzKTZnqtwQ'</td>\n",
       "      <td>i adore travis at the hard rocks new kelly car...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'yXQM5uF2jS6es16SJzNHfg'</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'GJXCdrto3ASJOqKeVWPi6Q'</td>\n",
       "      <td>[i, adore, travis, at, the, hard, rocks, new, ...</td>\n",
       "      <td>[, adore, travis, , , hard, rocks, new, kelly,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'2016-11-09 20:09:03'</td>\n",
       "      <td>3.0</td>\n",
       "      <td>b'WTqjgwHlXbSFevF32_DJVw'</td>\n",
       "      <td>i have to say that this office really has it t...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'n6-Gk65cPZL6Uz8qRm3NYw'</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'2TzJjDVDEuAW6MR5Vuc1ug'</td>\n",
       "      <td>[i, have, to, say, that, this, office, really,...</td>\n",
       "      <td>[, , , say, , , office, really, , , together, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'2018-01-09 20:56:38'</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'ikCg8xy5JIg_NGPx-MSIDA'</td>\n",
       "      <td>went in for a lunch steak sandwich was delicio...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'dacAIZ6fTM6mqwW5uxkskg'</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'yi0R0Ugj_xUx_Nek0-_Qig'</td>\n",
       "      <td>[went, in, for, a, lunch, steak, sandwich, was...</td>\n",
       "      <td>[went, , , , lunch, steak, sandwich, , delicio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'2018-01-30 23:07:38'</td>\n",
       "      <td>7.0</td>\n",
       "      <td>b'b1b1eb3uo-w561D0ZfCEiQ'</td>\n",
       "      <td>today was my second out of three sessions i ha...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'ssoyf2_x0EQMed6fgHeMyQ'</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'11a8sVPMUFtaC7_ABRkmtw'</td>\n",
       "      <td>[today, was, my, second, out, of, three, sessi...</td>\n",
       "      <td>[today, , , second, , , three, sessions, , , p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     date  useful                business_id  \\\n",
       "0  b'2013-05-07 04:34:36'     6.0  b'ujmEBvifdJM6h6RLv4wQIg'   \n",
       "1  b'2017-01-14 21:30:33'     0.0  b'NZnhc2sEQy3RmzKTZnqtwQ'   \n",
       "2  b'2016-11-09 20:09:03'     3.0  b'WTqjgwHlXbSFevF32_DJVw'   \n",
       "3  b'2018-01-09 20:56:38'     0.0  b'ikCg8xy5JIg_NGPx-MSIDA'   \n",
       "4  b'2018-01-30 23:07:38'     7.0  b'b1b1eb3uo-w561D0ZfCEiQ'   \n",
       "\n",
       "                                                text  funny  \\\n",
       "0  total bill for this horrible service over $8gs...    1.0   \n",
       "1  i adore travis at the hard rocks new kelly car...    0.0   \n",
       "2  i have to say that this office really has it t...    0.0   \n",
       "3  went in for a lunch steak sandwich was delicio...    0.0   \n",
       "4  today was my second out of three sessions i ha...    0.0   \n",
       "\n",
       "                     user_id  stars  cool                  review_id  \\\n",
       "0  b'hG7b0MtEbXx5QzbzE6C_VA'    1.0   0.0  b'Q1sbwvVQXV2734tPgoKj4Q'   \n",
       "1  b'yXQM5uF2jS6es16SJzNHfg'    5.0   0.0  b'GJXCdrto3ASJOqKeVWPi6Q'   \n",
       "2  b'n6-Gk65cPZL6Uz8qRm3NYw'    5.0   0.0  b'2TzJjDVDEuAW6MR5Vuc1ug'   \n",
       "3  b'dacAIZ6fTM6mqwW5uxkskg'    5.0   0.0  b'yi0R0Ugj_xUx_Nek0-_Qig'   \n",
       "4  b'ssoyf2_x0EQMed6fgHeMyQ'    1.0   0.0  b'11a8sVPMUFtaC7_ABRkmtw'   \n",
       "\n",
       "                                   tokenized_reviews  \\\n",
       "0  [total, bill, for, this, horrible, service, ov...   \n",
       "1  [i, adore, travis, at, the, hard, rocks, new, ...   \n",
       "2  [i, have, to, say, that, this, office, really,...   \n",
       "3  [went, in, for, a, lunch, steak, sandwich, was...   \n",
       "4  [today, was, my, second, out, of, three, sessi...   \n",
       "\n",
       "                                       no_stop_words  \n",
       "0  [total, bill, , , horrible, service, , $, 8gs,...  \n",
       "1  [, adore, travis, , , hard, rocks, new, kelly,...  \n",
       "2  [, , , say, , , office, really, , , together, ...  \n",
       "3  [went, , , , lunch, steak, sandwich, , delicio...  \n",
       "4  [today, , , second, , , three, sessions, , , p...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no_stop_words = remove_stop_words(reviews_train_clean)\n",
    "\n",
    "reviews['no_stop_words'] = reviews['tokenized_reviews'].apply(remove_stop_words)\n",
    "\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train = reviews[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_test = reviews[21:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/PythonData/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>useful</th>\n",
       "      <th>business_id</th>\n",
       "      <th>text</th>\n",
       "      <th>funny</th>\n",
       "      <th>user_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>cool</th>\n",
       "      <th>review_id</th>\n",
       "      <th>tokenized_reviews</th>\n",
       "      <th>no_stop_words</th>\n",
       "      <th>no_stop_words2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'2013-05-07 04:34:36'</td>\n",
       "      <td>6.0</td>\n",
       "      <td>b'ujmEBvifdJM6h6RLv4wQIg'</td>\n",
       "      <td>total bill for this horrible service over $8gs...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>b'hG7b0MtEbXx5QzbzE6C_VA'</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'Q1sbwvVQXV2734tPgoKj4Q'</td>\n",
       "      <td>[total, bill, for, this, horrible, service, ov...</td>\n",
       "      <td>[total, bill, , , horrible, service, , $, 8gs,...</td>\n",
       "      <td>[total, bill, , , horrible, service, , $, 8gs,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'2017-01-14 21:30:33'</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'NZnhc2sEQy3RmzKTZnqtwQ'</td>\n",
       "      <td>i adore travis at the hard rocks new kelly car...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'yXQM5uF2jS6es16SJzNHfg'</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'GJXCdrto3ASJOqKeVWPi6Q'</td>\n",
       "      <td>[i, adore, travis, at, the, hard, rocks, new, ...</td>\n",
       "      <td>[, adore, travis, , , hard, rocks, new, kelly,...</td>\n",
       "      <td>[, adore, travis, , , hard, rocks, new, kelly,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'2016-11-09 20:09:03'</td>\n",
       "      <td>3.0</td>\n",
       "      <td>b'WTqjgwHlXbSFevF32_DJVw'</td>\n",
       "      <td>i have to say that this office really has it t...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'n6-Gk65cPZL6Uz8qRm3NYw'</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'2TzJjDVDEuAW6MR5Vuc1ug'</td>\n",
       "      <td>[i, have, to, say, that, this, office, really,...</td>\n",
       "      <td>[, , , say, , , office, really, , , together, ...</td>\n",
       "      <td>[, , , say, , , office, really, , , together, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'2018-01-09 20:56:38'</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'ikCg8xy5JIg_NGPx-MSIDA'</td>\n",
       "      <td>went in for a lunch steak sandwich was delicio...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'dacAIZ6fTM6mqwW5uxkskg'</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'yi0R0Ugj_xUx_Nek0-_Qig'</td>\n",
       "      <td>[went, in, for, a, lunch, steak, sandwich, was...</td>\n",
       "      <td>[went, , , , lunch, steak, sandwich, , delicio...</td>\n",
       "      <td>[went, , , , lunch, steak, sandwich, , delicio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'2018-01-30 23:07:38'</td>\n",
       "      <td>7.0</td>\n",
       "      <td>b'b1b1eb3uo-w561D0ZfCEiQ'</td>\n",
       "      <td>today was my second out of three sessions i ha...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'ssoyf2_x0EQMed6fgHeMyQ'</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'11a8sVPMUFtaC7_ABRkmtw'</td>\n",
       "      <td>[today, was, my, second, out, of, three, sessi...</td>\n",
       "      <td>[today, , , second, , , three, sessions, , , p...</td>\n",
       "      <td>[today, , , second, , , three, sessions, , , p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     date  useful                business_id  \\\n",
       "0  b'2013-05-07 04:34:36'     6.0  b'ujmEBvifdJM6h6RLv4wQIg'   \n",
       "1  b'2017-01-14 21:30:33'     0.0  b'NZnhc2sEQy3RmzKTZnqtwQ'   \n",
       "2  b'2016-11-09 20:09:03'     3.0  b'WTqjgwHlXbSFevF32_DJVw'   \n",
       "3  b'2018-01-09 20:56:38'     0.0  b'ikCg8xy5JIg_NGPx-MSIDA'   \n",
       "4  b'2018-01-30 23:07:38'     7.0  b'b1b1eb3uo-w561D0ZfCEiQ'   \n",
       "\n",
       "                                                text  funny  \\\n",
       "0  total bill for this horrible service over $8gs...    1.0   \n",
       "1  i adore travis at the hard rocks new kelly car...    0.0   \n",
       "2  i have to say that this office really has it t...    0.0   \n",
       "3  went in for a lunch steak sandwich was delicio...    0.0   \n",
       "4  today was my second out of three sessions i ha...    0.0   \n",
       "\n",
       "                     user_id  stars  cool                  review_id  \\\n",
       "0  b'hG7b0MtEbXx5QzbzE6C_VA'    1.0   0.0  b'Q1sbwvVQXV2734tPgoKj4Q'   \n",
       "1  b'yXQM5uF2jS6es16SJzNHfg'    5.0   0.0  b'GJXCdrto3ASJOqKeVWPi6Q'   \n",
       "2  b'n6-Gk65cPZL6Uz8qRm3NYw'    5.0   0.0  b'2TzJjDVDEuAW6MR5Vuc1ug'   \n",
       "3  b'dacAIZ6fTM6mqwW5uxkskg'    5.0   0.0  b'yi0R0Ugj_xUx_Nek0-_Qig'   \n",
       "4  b'ssoyf2_x0EQMed6fgHeMyQ'    1.0   0.0  b'11a8sVPMUFtaC7_ABRkmtw'   \n",
       "\n",
       "                                   tokenized_reviews  \\\n",
       "0  [total, bill, for, this, horrible, service, ov...   \n",
       "1  [i, adore, travis, at, the, hard, rocks, new, ...   \n",
       "2  [i, have, to, say, that, this, office, really,...   \n",
       "3  [went, in, for, a, lunch, steak, sandwich, was...   \n",
       "4  [today, was, my, second, out, of, three, sessi...   \n",
       "\n",
       "                                       no_stop_words  \\\n",
       "0  [total, bill, , , horrible, service, , $, 8gs,...   \n",
       "1  [, adore, travis, , , hard, rocks, new, kelly,...   \n",
       "2  [, , , say, , , office, really, , , together, ...   \n",
       "3  [went, , , , lunch, steak, sandwich, , delicio...   \n",
       "4  [today, , , second, , , three, sessions, , , p...   \n",
       "\n",
       "                                      no_stop_words2  \n",
       "0  [total, bill, , , horrible, service, , $, 8gs,...  \n",
       "1  [, adore, travis, , , hard, rocks, new, kelly,...  \n",
       "2  [, , , say, , , office, really, , , together, ...  \n",
       "3  [went, , , , lunch, steak, sandwich, , delicio...  \n",
       "4  [today, , , second, , , three, sessions, , , p...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_train['no_stop_words2'] = list(filter(None, reviews_train['no_stop_words']))\n",
    "reviews_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-ebdb859f1e75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'no_stop_words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#X = tfidf_vectorizer.transform(reviews_train_clean)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/PythonData/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1359\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \"\"\"\n\u001b[0;32m-> 1361\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/PythonData/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 869\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/PythonData/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/PythonData/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 266\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/PythonData/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer.fit(reviews_train['no_stop_words'])\n",
    "#X = tfidf_vectorizer.transform(reviews_train_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
